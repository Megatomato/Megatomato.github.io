# A Deep Learning with Fastai Retrospective

When I recently took a machine learning course a semester ago, I remember having the thought: "How is anyone implementing this in code?!"  

To give you a sense of the level of maths and statistics that went into just the basics of machine learning, here are a couple examples of the formulas commonly seen:

For logistic regression, the likelihood function looks like this:

$$p(\mathbf{y} | \mathbf{w}, \mathbf{X}) = \prod_{i=1}^{N} p(y_i | x_i, \mathbf{w})$$

Which expands to:

$$= \prod_{i=1}^{N} \sigma(\mathbf{w}^T \mathbf{x}_i)^{y_i}(1 - \sigma(\mathbf{w}^T \mathbf{x}_i))^{1-y_i}$$

And then we typically optimize using the negative log-likelihood:

$$E(\mathbf{w}) = -\sum_{i=1}^{N} (y_i \log \sigma(\mathbf{w}^T \mathbf{x}_i) + (1 - y_i)\log(1 - \sigma(\mathbf{w}^T \mathbf{x}_i)))$$

As anyone can see, even trying to imagine how i'd implement this efficently and effectively in code gave me a headache. 

## Fastai: A Library That Abstracts the Complexity

The fastai library, built on top of PyTorch, was revolutionary in how it allowed even novices like me to train complex deep learning models. For example:

```python
from fastai.vision.all import *
learn = vision_learner(dls, resnet18, metrics=error_rate)
learn.fine_tune(5)
```

Just these three lines accomplish what would have taken hundreds of lines of custom code implementing not just the formulas above, but many more for a fully operational multi-class convolutional neural network. 

### Fastai customization 
Of course, just being very abstract doesn't mean that the fastai library can't be very customisable. Both the model and loss function are able to be chosen from a range of selections in the `Learner ` superclass (Or you can even design your own!). With just a few commands, you can change your dataset, batchsize, train/validation split:

```python
dls = DataBlock(
blocks=(ImageBlock, CategoryBlock),
get_items=get_image_files,
splitter=RandomSplitter(valid_pct=0.2, seed=42),
get_y=parent_label,
item_tfms=[Resize(192, method='squish')]
).dataloaders(path, bs=64)
```
It even gives you the option to create a confusion matrix for post-training analysis! 
```python
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix()
```




